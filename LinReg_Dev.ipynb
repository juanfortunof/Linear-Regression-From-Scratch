{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e498bab4",
   "metadata": {},
   "source": [
    "Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5bc37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "# import statsmodels.api as sm\n",
    "# import statsmodels.stats.diagnostic as smd\n",
    "# from statsmodels.stats.stattools import durbin_watson, jarque_bera\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1af0fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.fetch_california_housing()\n",
    "X = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "y = pd.DataFrame(data['target'], columns=['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de645389",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple_train = X[['AveBedrms']].head(16000)\n",
    "X_simple_test = X[['AveBedrms']].tail(4640)\n",
    "\n",
    "X_mult = X.copy()\n",
    "X_mult_train = X_mult.head(16000)\n",
    "X_mult_test = X_mult.tail(4640)\n",
    "\n",
    "y_train = y.head(16000)\n",
    "y_test = y.tail(4640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab49173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b33ac",
   "metadata": {},
   "source": [
    "Scikit Learn pred for Simple Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6ae774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Simple RMSE: 0.982420666235715\n",
      "Ridge Simple RMSE: 0.9828045748451621\n",
      "Lasso Simple RMSE: 0.9845237667499999\n"
     ]
    }
   ],
   "source": [
    "LR = LinearRegression()\n",
    "RR = Ridge(alpha=1000)\n",
    "LassoR = Lasso(alpha=1000)\n",
    "\n",
    "LR.fit(X_simple_train, y_train)\n",
    "LR_pred = LR.predict(X_simple_test)\n",
    "\n",
    "RR.fit(X_simple_train, y_train)\n",
    "RR_pred = RR.predict(X_simple_test)\n",
    "\n",
    "LassoR.fit(X_simple_train, y_train)\n",
    "LassoR_pred = LassoR.predict(X_simple_test)\n",
    "\n",
    "print(f'Linear Simple RMSE: {mean_absolute_error(y_test, LR_pred)}')\n",
    "print(f'Ridge Simple RMSE: {mean_absolute_error(y_test, RR_pred)}')\n",
    "print(f'Lasso Simple RMSE: {mean_absolute_error(y_test, LassoR_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4dec16",
   "metadata": {},
   "source": [
    "Scikit Learn pred for Mult Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b426614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Simple RMSE: 0.5183922114609337\n",
      "Ridge Simple RMSE: 0.52261892925488\n",
      "Lasso Simple RMSE: 0.9845237667499999\n"
     ]
    }
   ],
   "source": [
    "LR.fit(X_mult_train, y_train)\n",
    "LR_pred_mult = LR.predict(X_mult_test)\n",
    "\n",
    "RR.fit(X_mult_train, y_train)\n",
    "RR_pred_mult = RR.predict(X_mult_test)\n",
    "\n",
    "LassoR.fit(X_mult_train, y_train)\n",
    "LassoR_pred_mult = LassoR.predict(X_mult_test)\n",
    "\n",
    "print(f'Linear Simple RMSE: {mean_absolute_error(y_test, LR_pred_mult)}')\n",
    "print(f'Ridge Simple RMSE: {mean_absolute_error(y_test, RR_pred_mult)}')\n",
    "print(f'Lasso Simple RMSE: {mean_absolute_error(y_test, LassoR_pred_mult)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "26bd8f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 11)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pru_aux_R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "95b96672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\2405442057.py:24: UserWarning: Ramsey Test failed, your model has non-linear relations, try using polynomial or logarithmic convertions.\n",
      "  warnings.warn('Ramsey Test failed, your model has non-linear relations, try using polynomial or logarithmic convertions.')\n"
     ]
    }
   ],
   "source": [
    "def RamseyReset(X, y):\n",
    "\n",
    "    all_w = LinearRegressor._get_weights(X, y)\n",
    "    w1, b1 = all_w[0:-1], all_w[-1]\n",
    "    pred1 = X @ w1 + b1\n",
    "    pred1_sq = pred_pru ** 2\n",
    "    pred1_cu = pred_pru ** 3\n",
    "    ssrr = np.sum((pred1 - y)**2, axis=0)\n",
    "\n",
    "    X2 = np.hstack([X, pred1_sq, pred1_cu])\n",
    "    all_w_R = LinearRegressor._get_weights(X2, y)\n",
    "    w2, b2 = all_w_R[0:-1], all_w_R[-1]\n",
    "    pred2 = X2 @ w2 + b2\n",
    "    ssra = np.sum((pred2 - y)**2, axis=0)\n",
    "\n",
    "    q = 2\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "\n",
    "    F = ((ssrr-ssra) / q) / (ssra / (n - k - q - 1)) \n",
    "    F = F[0]\n",
    "\n",
    "    if F > 3:\n",
    "        warnings.warn('Ramsey Test failed, your model has non-linear relations, try using polynomial or logarithmic convertions.')\n",
    "\n",
    "RamseyReset(X_mult_train.values, y_train.values)\n",
    "Fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301928ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8705042151218245\n"
     ]
    }
   ],
   "source": [
    "def check_DW(X, y):\n",
    "\n",
    "    all_w = LinearRegressor._get_weights(X, y)\n",
    "    w, b = all_w[:-1], all_w[-1]\n",
    "\n",
    "    pred = X @ w + b\n",
    "    res = y - pred\n",
    "    d = np.sum((res[:-1] - res[1:])**2) / np.sum(res **2)\n",
    "\n",
    "    \n",
    "\n",
    "check_DW(X_mult_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "08422114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9389904335366368"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.f.ppf(1-0.05, 8, 16000-8-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "40daca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\2207201497.py:24: UserWarning: The BP test shows that your model has heterocedaskicity problems, try transforming the dependent and independent variables.\n",
      "  warnings.warn('The BP test shows that your model has heterocedaskicity problems, try transforming the dependent and independent variables.')\n"
     ]
    }
   ],
   "source": [
    "def check_Heterocedaskicity(X, y):\n",
    "\n",
    "    all_w = LinearRegressor._get_weights(X, y)\n",
    "    w, b = all_w[:-1], all_w[-1]\n",
    "    pred = X @ w + b\n",
    "    res = y - pred\n",
    "    res_sq = res**2\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "    ssr = np.sum(res_sq)\n",
    "\n",
    "    var_res = ssr / (n-k-1)\n",
    "    g =  res_sq / var_res\n",
    "\n",
    "    all_w2 = LinearRegressor._get_weights(X, g)\n",
    "    w2, b2 = all_w2[:-1], all_w2[-1]\n",
    "    pred2 = X @ w2 + b2\n",
    "\n",
    "    R2 = Metrics.get_R2(pred2, g)\n",
    "    LM = n * R2\n",
    "    chi_square_value = scipy.stats.chi2.ppf(1-0.05, k)\n",
    "\n",
    "    if LM > chi_square_value:\n",
    "        warnings.warn('The BP test shows that your model has heterocedaskicity problems, try transforming the dependent and independent variables.')\n",
    "        \n",
    "check_Heterocedaskicity(X_mult_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "152ac459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estadístico Jarque-Bera: 13584.7122\n",
      "13584.71215673286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\4063093287.py:46: UserWarning: Jarque Bera test failed. The residuals are not normally distributed (JB > Chi^2_crit).\n",
      "  warnings.warn('Jarque Bera test failed. The residuals are not normally distributed (JB > Chi^2_crit).', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from scipy.stats import chi2 # Necesaria para el valor crítico\n",
    "\n",
    "def test_JB_Corregido(X, y):\n",
    "\n",
    "    # 1. Regresión y Residuos\n",
    "    all_w = LinearRegressor._get_weights(X, y)\n",
    "    w, b = all_w[:-1], all_w[-1]\n",
    "    pred = X @ w + b\n",
    "    res = y - pred\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # La media de residuos en MCO es ~0, pero se incluye por robustez\n",
    "    res_mean = np.mean(res) \n",
    "    \n",
    "    # --- 2. Momentos Centrales y Normalización ---\n",
    "    \n",
    "    # 2.1. Varianza (Momento central de orden 2)\n",
    "    M2 = np.sum((res - res_mean)**2) / n # Varianza muestral\n",
    "    \n",
    "    # Desviación estándar al cuadrado y al cubo para la normalización\n",
    "    std_dev = np.sqrt(M2)\n",
    "    \n",
    "    # 2.2. Asimetría (S)\n",
    "    M3 = np.sum((res - res_mean)**3) / n # Momento central de orden 3\n",
    "    S = M3 / (std_dev**3) # M3 normalizado por la desviación estándar al cubo\n",
    "    \n",
    "    # 2.3. Curtosis (K)\n",
    "    M4 = np.sum((res - res_mean)**4) / n # Momento central de orden 4\n",
    "    K = M4 / (std_dev**4) # M4 normalizado por la desviación estándar a la cuarta\n",
    "    \n",
    "    # --- 3. Estadístico JB ---\n",
    "    \n",
    "    # Fórmula correcta: utiliza la curtosis excesiva (K - 3)\n",
    "    JB = n / 6 * ((S**2) + ((K - 3)**2 / 4)) # <--- CORRECCIÓN CLAVE\n",
    "    \n",
    "    # --- 4. Evaluación ---\n",
    "    \n",
    "    # El valor crítico Chi-cuadrado para alpha=0.05 y GL=2 es ~5.991\n",
    "    JB_critic = chi2.ppf(1 - 0.05, 2)\n",
    "    \n",
    "    print(f\"Estadístico Jarque-Bera: {JB:.4f}\")\n",
    "    \n",
    "    if JB > JB_critic:\n",
    "        warnings.warn('Jarque Bera test failed. The residuals are not normally distributed (JB > Chi^2_crit).', UserWarning)\n",
    "    else:\n",
    "        print(\"Test de Jarque-Bera Aprobado. Los residuos son consistentes con una distribución normal.\")\n",
    "        \n",
    "    print(JB)\n",
    "\n",
    "test_JB_Corregido(X_mult_train.values, y_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56663fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13584.71215673286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\419903723.py:24: UserWarning: Jarque Bera test failed. The residuals are not normally distributed.\n",
      "  warnings.warn('Jarque Bera test failed. The residuals are not normally distributed.')\n"
     ]
    }
   ],
   "source": [
    "def test_JB(X, y):\n",
    "\n",
    "    all_w = LinearRegressor._get_weights(X, y)\n",
    "    w, b = all_w[:-1], all_w[-1]\n",
    "\n",
    "    pred = X @ w + b\n",
    "    res = y - pred\n",
    "    n = X.shape[0]\n",
    "    res_mean = np.mean(res)\n",
    "\n",
    "    M2 = 1/n * np.sum((res-res_mean)**2)\n",
    "    M3 = 1/n * np.sum((res-res_mean)**3)\n",
    "    M4 = 1/n * np.sum((res-res_mean)**4)\n",
    "    std_dev = np.sqrt(M2)\n",
    "    \n",
    "    S = M3 / std_dev**3\n",
    "    K = M4 / std_dev**4\n",
    "\n",
    "    JB = n/6 * ((S**2) + ((K - 3)**2 / 4))\n",
    "\n",
    "    JB_critic = scipy.stats.chi2.ppf(1-0.05, 2)\n",
    "\n",
    "    if JB > JB_critic:\n",
    "        warnings.warn('Jarque Bera test failed. The residuals are not normally distributed.')\n",
    "\n",
    "\n",
    "test_JB(X_mult_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f2f08",
   "metadata": {},
   "source": [
    "Now my implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a08ab2",
   "metadata": {},
   "source": [
    "Cosas a corregir del modelo.\n",
    "\n",
    "1. Divide y conquista: Tienes todo en una sola clase, eso lo hace dificil de mantener, trata de separar cada etapa.\n",
    "2. Trata de que todo este escrito con Numpy, en vez de recibir dataframes de pandas, trata de que solo se pueda recibir arrays de numpy y trabajar con ellos, o por lo menos separar o convertir al inicio y luego hacer todo el modelo con puro numpy.\n",
    "3. Separa las metricas y puedes crear una clase solo para ellas, de modo que sea mas legible y facil de mantener\n",
    "4. Que los tests del modelo sean netamente producto del modelo y no de otras librerias, sino pierde sentido porque no es \"desde cero\".\n",
    "5. Tu regresor tiene que ser a prueba de balas, preparalo para NaNs, tipos de datos coherentes, dimensiones de arrays correctas, variables categoricas.\n",
    "6. Usa Warnings en vez de prints, los print no sirven para produccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5df564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Validator:\n",
    "\n",
    "#     def validate(self, X, y):\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_data(self, X: pd.DataFrame | pd.Series, y: pd.Series, train: bool=True) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        if isinstance(X, pd.Series):\n",
    "            X = X.values.reshape(-1, 1)\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        if train:\n",
    "            if isinstance(y, pd.Series):\n",
    "                y = y.values.reshape(-1, 1)\n",
    "            elif isinstance(y, pd.DataFrame):\n",
    "                y = y.values\n",
    "\n",
    "            return X, y\n",
    "        \n",
    "        return X\n",
    "    \n",
    "\n",
    "class CorrChecker:\n",
    "\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, feature_names:list):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    @classmethod\n",
    "    def _check_corr(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "\n",
    "        X = pd.DataFrame(X, columns=self.feature_names)\n",
    "        y = pd.DataFrame(y, columns=['Target'])\n",
    "\n",
    "        df = pd.concat([X, y], axis=1)\n",
    "        corrs = df.corr().iloc[:, -1]\n",
    "\n",
    "        for col, corr in zip(corrs.index, corrs):\n",
    "            if abs(corr) < 0.3:\n",
    "                print(f'WARNING! {col} has a low correlation with the target variable ({np.round(corr, 2)}).')\n",
    "\n",
    "\n",
    "class LinearRegressor:\n",
    "\n",
    "    def __init__(self, w=None, b=None, TimeSeries=False):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.TimeSeries = TimeSeries\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_weights(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        b = np.ones(shape=(X.shape[0], 1))\n",
    "        input_b = np.hstack((X, b))\n",
    "\n",
    "        weights = (np.linalg.inv(input_b.T @ input_b) @ input_b.T) @ y\n",
    "\n",
    "        return weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_residuals(pred: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        ''' This is a private method '''\n",
    "\n",
    "        residuals = pred - y\n",
    "        return residuals\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame | pd.Series, y_train: pd.Series) -> None:\n",
    "\n",
    "        '''\n",
    "            This method acts like training the model, it will also tell you if your\n",
    "            model has heterocedaskicity, autocorrelation, multicolinearity, is it\n",
    "            bad specified and if the data is normally distributed.\n",
    "        '''\n",
    "\n",
    "        CorrChecker._check_corr(X_train, y_train)\n",
    "        X_train_r, y_train_r = Preprocessor._convert_data(X_train, y_train)\n",
    "        w_b = self._get_weights(X_train_r, y_train_r)\n",
    "        self.w = w_b[:-1]\n",
    "        self.b = w_b[-1]\n",
    "        print()\n",
    "        \n",
    "        AssumpChecker._check_assumptions(X_train_r, y_train_r, self.TimeSeries)\n",
    "\n",
    "        if len(X_train.shape) > 1:\n",
    "            AssumpChecker._check_multicol(X_train)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame | pd.Series) -> np.ndarray:\n",
    "\n",
    "        '''\n",
    "            This method predicts the test set using the weights and the Bias.\n",
    "        '''\n",
    "\n",
    "        X = Preprocessor._convert_data(X, np.array([1]), train=False)\n",
    "\n",
    "        pred = X @ self.w + self.b\n",
    "        return pred\n",
    "    \n",
    "    \n",
    "class AssumpChecker:\n",
    "\n",
    "    @staticmethod\n",
    "    def __check_Ramsey(X: np.ndarray, y: np.ndarray) -> None:\n",
    "\n",
    "        ''' \n",
    "            This method does all the Ramsey process from start to finish, it\n",
    "            can be hard to understand, so I suggest supporting the explanations\n",
    "            with AI.\n",
    "        '''\n",
    "\n",
    "        all_w = LinearRegressor._get_weights(X, y)\n",
    "        w1, b1 = all_w[0:-1], all_w[-1]\n",
    "        pred1 = X @ w1 + b1\n",
    "        pred1_sq = pred_pru ** 2\n",
    "        pred1_cu = pred_pru ** 3\n",
    "        ssrr = np.sum((pred1 - y)**2, axis=0)\n",
    "\n",
    "        X2 = np.hstack([X, pred1_sq, pred1_cu])\n",
    "        all_w_R = LinearRegressor._get_weights(X2, y)\n",
    "        w2, b2 = all_w_R[0:-1], all_w_R[-1]\n",
    "        pred2 = X2 @ w2 + b2\n",
    "        ssra = np.sum((pred2 - y)**2, axis=0)\n",
    "\n",
    "        q = 2\n",
    "        n = X.shape[0]\n",
    "        k = X.shape[1]\n",
    "\n",
    "        F = ((ssrr-ssra) / q) / (ssra / (n - k - q - 1)) \n",
    "        F = F[0]\n",
    "        F_critic = scipy.stats.F.ppf(1-0.05, k, n-k-q-1)\n",
    "\n",
    "        if F > F_critic:\n",
    "            warnings.warn('Ramsey Test failed, your model has non-linear relations, try using polynomial or logarithmic convertions.')\n",
    "\n",
    "    @staticmethod\n",
    "    def __check_dw(X: np.ndarray, y: np.ndarray) -> None:\n",
    "\n",
    "        ''' This method just applies for Time Series data only, it does the Durbin Watson test. '''\n",
    "\n",
    "        all_w = LinearRegressor._get_weights(X, y)\n",
    "        w, b = all_w[:-1], all_w[-1]\n",
    "\n",
    "        pred = X @ w + b\n",
    "        res = y - pred\n",
    "        d = np.sum((res[:-1] - res[1:])**2) / np.sum(res **2)\n",
    "\n",
    "        if d < 1.8:\n",
    "            warnings.warn('Durbin Watson test shows that the model has positive correlation problems, try adding lags of one of the dependent variables as another dependent variable.')\n",
    "        elif d > 2.2:\n",
    "            warnings.warn('Durbin Watson test shows that the model has negative correlation problems, try adding lags of one of the dependent variables as another dependent variable.')\n",
    "\n",
    "    @staticmethod\n",
    "    def __check_ht(X,: np.ndarray, y: np.ndarray) -> None:\n",
    "\n",
    "        ''' This test checks for Heterocedaskicity on your model.'''\n",
    "\n",
    "        all_w = LinearRegressor._get_weights(X, y)\n",
    "        w, b = all_w[:-1], all_w[-1]\n",
    "        pred = X @ w + b\n",
    "        res = y - pred\n",
    "        res_sq = res**2\n",
    "        n = X.shape[0]\n",
    "        k = X.shape[1]\n",
    "        ssr = np.sum(res_sq)\n",
    "\n",
    "        var_res = ssr / (n-k-1)\n",
    "        g =  res_sq / var_res\n",
    "\n",
    "        all_w2 = LinearRegressor._get_weights(X, g)\n",
    "        w2, b2 = all_w2[:-1], all_w2[-1]\n",
    "        pred2 = X @ w2 + b2\n",
    "\n",
    "        R2 = Metrics.get_R2(pred2, g)\n",
    "        LM = n * R2\n",
    "        chi_square_value = scipy.stats.chi2.ppf(1-0.05, k)\n",
    "\n",
    "        if LM > chi_square_value:\n",
    "            warnings.warn('The BP test shows that your model has heterocedaskicity problems, try transforming the dependent and independent variables.')\n",
    "\n",
    "    @staticmethod\n",
    "    def __check_jb(X: np.ndarray, y: np.ndarray) -> None:\n",
    "\n",
    "        ''' This is a private method '''\n",
    "        \n",
    "        all_w = LinearRegressor._get_weights(X, y)\n",
    "        w, b = all_w[:-1], all_w[-1]\n",
    "\n",
    "        pred = X @ w + b\n",
    "        res = y - pred\n",
    "        n = X.shape[0]\n",
    "        res_mean = np.mean(res)\n",
    "\n",
    "        M2 = 1/n * np.sum((res-res_mean)**2)\n",
    "        M3 = 1/n * np.sum((res-res_mean)**3)\n",
    "        M4 = 1/n * np.sum((res-res_mean)**4)\n",
    "        std_dev = np.sqrt(M2)\n",
    "        \n",
    "        S = M3 / std_dev**3\n",
    "        K = M4 / std_dev**4\n",
    "\n",
    "        JB = n/6 * ((S**2) + ((K - 3)**2 / 4))\n",
    "\n",
    "        JB_critic = scipy.stats.chi2.ppf(1-0.05, 2)\n",
    "\n",
    "        if JB > JB_critic:\n",
    "            warnings.warn('Jarque Bera test failed. The residuals are not normally distributed.')\n",
    "\n",
    "    @staticmethod\n",
    "    def __check_multicol(X: pd.DataFrame) -> None:\n",
    "\n",
    "        ''' This is a private method '''\n",
    "\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data['feature'] = X.columns\n",
    "        vif_data['VIF'] = [variance_inflation_factor(X.values, i) \\\n",
    "                               for i in range(X.shape[1])]\n",
    "\n",
    "        for idx, row in vif_data.iterrows():\n",
    "            if row['VIF'] >= 5:\n",
    "                print(f'WARNING! Check the correlation between {row['feature']} and the other independent variables. VIF: {np.round(row['VIF'], 2)}')\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_assumptions(X: np.ndarray, y: np.ndarray, TimeSeries: bool = False) -> None:\n",
    "\n",
    "        ''' This is a private method '''\n",
    "\n",
    "        jb = jarque_bera(residuals)\n",
    "        AssumpChecker.__check_Ramsey(X, y)\n",
    "        AssumpChecker.__check_jb(X, y)\n",
    "        AssumpChecker.__check_ht(X, y)\n",
    "\n",
    "        if TimeSeries:\n",
    "            AssumpChecker.__check_dw(X, y)\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_MSE(pred: np.ndarray, y: np.ndarray) -> float:\n",
    "\n",
    "        ''' This method calculates the MSE '''\n",
    "\n",
    "        m = pred.shape[0]\n",
    "\n",
    "        MSE = (1/m) * np.sum((pred - y) ** 2)\n",
    "        return MSE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_RMSE(pred: np.ndarray, y: np.ndarray) -> float:\n",
    "\n",
    "        ''' This method calculates the RMSE '''\n",
    "\n",
    "        m = pred.shape[0]\n",
    "\n",
    "        RMSE = ((1/m) * np.sum((pred - y) ** 2)) ** 0.5\n",
    "        return RMSE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_MAE(pred: np.ndarray, y: np.ndarray) -> float:\n",
    "\n",
    "        ''' This method calculates the MAE '''\n",
    "\n",
    "        m = pred.shape[0]\n",
    "\n",
    "        MAE = 1/m * (np.sum(abs(pred - y)))\n",
    "        return MAE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_R2(pred: np.ndarray, y: np.ndarray) -> float:\n",
    "\n",
    "        ''' This method calculates the R2 '''\n",
    "\n",
    "        num = np.sum((pred - y.mean()) ** 2)\n",
    "        den = np.sum((y - y.mean()) ** 2)\n",
    "        R2 = 1 - (num / den)\n",
    "\n",
    "        return R2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class LinearRegressor:\n",
    "\n",
    "    def __init__(self, w=None, b=None):\n",
    "        self.__w = w\n",
    "        self.__b = b\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def __get_regplot(X: np.ndarray, pred: np.ndarray, y: np.ndarray) -> None:\n",
    "\n",
    "        ''' This is a private method '''\n",
    "\n",
    "        plt.scatter(x=y, y=X, alpha=0.3)\n",
    "        plt.plot(pred, X, c='r')\n",
    "        plt.title('Regresion plot')\n",
    "        plt.xlabel('Input')\n",
    "        plt.ylabel('Actual Values')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_residual_plot(pred: np.ndarray, residuals: np.ndarray) -> None:\n",
    "\n",
    "        ''' This is a private method '''\n",
    "\n",
    "        plt.scatter(pred, residuals)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel('Predictions')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.title('Residuals Plot')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def get_metrics_report(self,\n",
    "                           X: pd.DataFrame | pd.Series,\n",
    "                           pred: np.ndarray,\n",
    "                           y: np.ndarray,\n",
    "                           charts=True) -> None:\n",
    "\n",
    "        '''\n",
    "            This method will give you a quick report of you regression, showing you metrics\n",
    "            like RMSE, MSE, MAE, R2, a regression plot and a residual plot.\n",
    "        '''\n",
    "\n",
    "        X_res, y_res = LinearRegressor.__convert_training_data(X, y)\n",
    "\n",
    "        RMSE = self.get_RMSE(pred, y_res)\n",
    "        MSE = self.get_MSE(pred, y_res)\n",
    "        MAE = self.get_MAE(pred, y_res)\n",
    "        R2 = self.get_R2(pred, y_res)\n",
    "        residuals = LinearRegressor.__get_residuals(pred, y_res)\n",
    "\n",
    "        print()\n",
    "        print('------------------METRICS REPORT-----------------\\n')\n",
    "        print(f'MSE: {MSE}\\nRMSE: {RMSE}\\nMAE: {MAE}\\nR2: {R2}\\n')\n",
    "\n",
    "        if charts:\n",
    "\n",
    "            if X_res.shape[1] == 1:\n",
    "                LinearRegressor.__get_regplot(X_res, pred, y_res)\n",
    "                print()\n",
    "\n",
    "            LinearRegressor.__get_residual_plot(pred, residuals)\n",
    "\n",
    "\n",
    "class RidgeRegressor(LinearRegressor):\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _get_weights(self, input: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        b = np.ones(shape=(input.shape[0], 1))\n",
    "        input_b = np.hstack((input, b))\n",
    "\n",
    "        weights = (np.linalg.inv((input_b.T @ input_b) + self.alpha * np.eye(input_b.shape[1])) @ input_b.T) @ y\n",
    "\n",
    "        return weights\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
