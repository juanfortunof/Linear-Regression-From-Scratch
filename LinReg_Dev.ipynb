{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e498bab4",
   "metadata": {},
   "source": [
    "Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5bc37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "# import statsmodels.api as sm\n",
    "# import statsmodels.stats.diagnostic as smd\n",
    "# from statsmodels.stats.stattools import durbin_watson, jarque_bera\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1af0fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "data = datasets.fetch_california_housing()\n",
    "X = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "y = pd.DataFrame(data['target'], columns=['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de645389",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple_train = X[['AveBedrms']].head(16000)\n",
    "X_simple_test = X[['AveBedrms']].tail(4640)\n",
    "\n",
    "X_mult = X.copy()\n",
    "X_mult_train = X_mult.head(16000)\n",
    "X_mult_test = X_mult.tail(4640)\n",
    "\n",
    "y_train = y.head(16000)\n",
    "y_test = y.tail(4640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab49173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4dec16",
   "metadata": {},
   "source": [
    "Scikit Learn pred for Mult Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "26bd8f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 11)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pru_aux_R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "95b96672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\2405442057.py:24: UserWarning: Ramsey Test failed, your model has non-linear relations, try using polynomial or logarithmic convertions.\n",
      "  warnings.warn('Ramsey Test failed, your model has non-linear relations, try using polynomial or logarithmic convertions.')\n"
     ]
    }
   ],
   "source": [
    "def RamseyReset(X, y):\n",
    "\n",
    "    all_w = LinearRegressor._get_weights(X, y)\n",
    "    w1, b1 = all_w[0:-1], all_w[-1]\n",
    "    pred1 = X @ w1 + b1\n",
    "    pred1_sq = pred_pru ** 2\n",
    "    pred1_cu = pred_pru ** 3\n",
    "    ssrr = np.sum((pred1 - y)**2, axis=0)\n",
    "\n",
    "    X2 = np.hstack([X, pred1_sq, pred1_cu])\n",
    "    all_w_R = LinearRegressor._get_weights(X2, y)\n",
    "    w2, b2 = all_w_R[0:-1], all_w_R[-1]\n",
    "    pred2 = X2 @ w2 + b2\n",
    "    ssra = np.sum((pred2 - y)**2, axis=0)\n",
    "\n",
    "    q = 2\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "\n",
    "    F = ((ssrr-ssra) / q) / (ssra / (n - k - q - 1)) \n",
    "    F = F[0]\n",
    "\n",
    "    if F > 3:\n",
    "        warnings.warn('Ramsey Test failed, your model has non-linear relations, try using polynomial or logarithmic convertions.')\n",
    "\n",
    "RamseyReset(X_mult_train.values, y_train.values)\n",
    "Fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301928ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8705042151218245\n"
     ]
    }
   ],
   "source": [
    "def check_DW(X, y):\n",
    "\n",
    "    all_w = LinearRegressor._get_weights(X, y)\n",
    "    w, b = all_w[:-1], all_w[-1]\n",
    "\n",
    "    pred = X @ w + b\n",
    "    res = y - pred\n",
    "    d = np.sum((res[:-1] - res[1:])**2) / np.sum(res **2)\n",
    "\n",
    "    \n",
    "\n",
    "check_DW(X_mult_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "08422114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9389904335366368"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.f.ppf(1-0.05, 8, 16000-8-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "40daca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\2207201497.py:24: UserWarning: The BP test shows that your model has heterocedaskicity problems, try transforming the dependent and independent variables.\n",
      "  warnings.warn('The BP test shows that your model has heterocedaskicity problems, try transforming the dependent and independent variables.')\n"
     ]
    }
   ],
   "source": [
    "def check_Heterocedaskicity(X, y):\n",
    "\n",
    "    all_w = LinearRegressor._get_weights(X, y)\n",
    "    w, b = all_w[:-1], all_w[-1]\n",
    "    pred = X @ w + b\n",
    "    res = y - pred\n",
    "    res_sq = res**2\n",
    "    n = X.shape[0]\n",
    "    k = X.shape[1]\n",
    "    ssr = np.sum(res_sq)\n",
    "\n",
    "    var_res = ssr / (n-k-1)\n",
    "    g =  res_sq / var_res\n",
    "\n",
    "    all_w2 = LinearRegressor._get_weights(X, g)\n",
    "    w2, b2 = all_w2[:-1], all_w2[-1]\n",
    "    pred2 = X @ w2 + b2\n",
    "\n",
    "    R2 = Metrics.get_R2(pred2, g)\n",
    "    LM = n * R2\n",
    "    chi_square_value = scipy.stats.chi2.ppf(1-0.05, k)\n",
    "\n",
    "    if LM > chi_square_value:\n",
    "        warnings.warn('The BP test shows that your model has heterocedaskicity problems, try transforming the dependent and independent variables.')\n",
    "        \n",
    "check_Heterocedaskicity(X_mult_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "152ac459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estadístico Jarque-Bera: 13584.7122\n",
      "13584.71215673286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\4063093287.py:46: UserWarning: Jarque Bera test failed. The residuals are not normally distributed (JB > Chi^2_crit).\n",
      "  warnings.warn('Jarque Bera test failed. The residuals are not normally distributed (JB > Chi^2_crit).', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from scipy.stats import chi2 # Necesaria para el valor crítico\n",
    "\n",
    "def test_JB_Corregido(X, y):\n",
    "\n",
    "    # 1. Regresión y Residuos\n",
    "    all_w = LinearRegressor._get_weights(X, y)\n",
    "    w, b = all_w[:-1], all_w[-1]\n",
    "    pred = X @ w + b\n",
    "    res = y - pred\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # La media de residuos en MCO es ~0, pero se incluye por robustez\n",
    "    res_mean = np.mean(res) \n",
    "    \n",
    "    # --- 2. Momentos Centrales y Normalización ---\n",
    "    \n",
    "    # 2.1. Varianza (Momento central de orden 2)\n",
    "    M2 = np.sum((res - res_mean)**2) / n # Varianza muestral\n",
    "    \n",
    "    # Desviación estándar al cuadrado y al cubo para la normalización\n",
    "    std_dev = np.sqrt(M2)\n",
    "    \n",
    "    # 2.2. Asimetría (S)\n",
    "    M3 = np.sum((res - res_mean)**3) / n # Momento central de orden 3\n",
    "    S = M3 / (std_dev**3) # M3 normalizado por la desviación estándar al cubo\n",
    "    \n",
    "    # 2.3. Curtosis (K)\n",
    "    M4 = np.sum((res - res_mean)**4) / n # Momento central de orden 4\n",
    "    K = M4 / (std_dev**4) # M4 normalizado por la desviación estándar a la cuarta\n",
    "    \n",
    "    # --- 3. Estadístico JB ---\n",
    "    \n",
    "    # Fórmula correcta: utiliza la curtosis excesiva (K - 3)\n",
    "    JB = n / 6 * ((S**2) + ((K - 3)**2 / 4)) # <--- CORRECCIÓN CLAVE\n",
    "    \n",
    "    # --- 4. Evaluación ---\n",
    "    \n",
    "    # El valor crítico Chi-cuadrado para alpha=0.05 y GL=2 es ~5.991\n",
    "    JB_critic = chi2.ppf(1 - 0.05, 2)\n",
    "    \n",
    "    print(f\"Estadístico Jarque-Bera: {JB:.4f}\")\n",
    "    \n",
    "    if JB > JB_critic:\n",
    "        warnings.warn('Jarque Bera test failed. The residuals are not normally distributed (JB > Chi^2_crit).', UserWarning)\n",
    "    else:\n",
    "        print(\"Test de Jarque-Bera Aprobado. Los residuos son consistentes con una distribución normal.\")\n",
    "        \n",
    "    print(JB)\n",
    "\n",
    "test_JB_Corregido(X_mult_train.values, y_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56663fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13584.71215673286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\419903723.py:24: UserWarning: Jarque Bera test failed. The residuals are not normally distributed.\n",
      "  warnings.warn('Jarque Bera test failed. The residuals are not normally distributed.')\n"
     ]
    }
   ],
   "source": [
    "def test_JB(X, y):\n",
    "\n",
    "    all_w = LinearRegressor._get_weights(X, y)\n",
    "    w, b = all_w[:-1], all_w[-1]\n",
    "\n",
    "    pred = X @ w + b\n",
    "    res = y - pred\n",
    "    n = X.shape[0]\n",
    "    res_mean = np.mean(res)\n",
    "\n",
    "    M2 = 1/n * np.sum((res-res_mean)**2)\n",
    "    M3 = 1/n * np.sum((res-res_mean)**3)\n",
    "    M4 = 1/n * np.sum((res-res_mean)**4)\n",
    "    std_dev = np.sqrt(M2)\n",
    "    \n",
    "    S = M3 / std_dev**3\n",
    "    K = M4 / std_dev**4\n",
    "\n",
    "    JB = n/6 * ((S**2) + ((K - 3)**2 / 4))\n",
    "\n",
    "    JB_critic = scipy.stats.chi2.ppf(1-0.05, 2)\n",
    "\n",
    "    if JB > JB_critic:\n",
    "        warnings.warn('Jarque Bera test failed. The residuals are not normally distributed.')\n",
    "\n",
    "\n",
    "test_JB(X_mult_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3b2b5451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\1610847744.py:19: UserWarning: Multicolinearity test failed, This is a serious issue, check the correlation between your features, they should not be correlated.\n",
      "  warnings.warn('Multicolinearity test failed, This is a serious issue, check the correlation between your features, they should not be correlated.')\n"
     ]
    }
   ],
   "source": [
    "def check_multicolinearity(X):\n",
    "\n",
    "    VIFs = {col:0 for col in X.columns}\n",
    "\n",
    "    for col in X.columns:\n",
    "        X_aux = X.drop(col, axis=1)\n",
    "        y = X[col]\n",
    "\n",
    "        all_w = LinearRegressor._get_weights(X_aux, y)\n",
    "        w, b = all_w[:-1], all_w[-1]\n",
    "        pred = X_aux @ w + b\n",
    "        R2_aux = Metrics.get_R2(pred, y)\n",
    "\n",
    "        VIFs[col] = 1 / (1 - R2_aux)\n",
    "\n",
    "    for vif in VIFs.values():\n",
    "\n",
    "        if vif > 5:\n",
    "            warnings.warn('Multicolinearity test failed, This is a serious issue, check the correlation between your features, they should not be correlated.')\n",
    "            break\n",
    "\n",
    "check_multicolinearity(X_mult_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f2f08",
   "metadata": {},
   "source": [
    "Now my implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a08ab2",
   "metadata": {},
   "source": [
    "Cosas a corregir del modelo.\n",
    "\n",
    "1. Divide y conquista: Tienes todo en una sola clase, eso lo hace dificil de mantener, trata de separar cada etapa.\n",
    "2. Trata de que todo este escrito con Numpy, en vez de recibir dataframes de pandas, trata de que solo se pueda recibir arrays de numpy y trabajar con ellos, o por lo menos separar o convertir al inicio y luego hacer todo el modelo con puro numpy.\n",
    "3. Separa las metricas y puedes crear una clase solo para ellas, de modo que sea mas legible y facil de mantener\n",
    "4. Que los tests del modelo sean netamente producto del modelo y no de otras librerias, sino pierde sentido porque no es \"desde cero\".\n",
    "5. Tu regresor tiene que ser a prueba de balas, preparalo para NaNs, tipos de datos coherentes, dimensiones de arrays correctas, variables categoricas.\n",
    "6. Usa Warnings en vez de prints, los print no sirven para produccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1b14b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Simple MAE: 0.982420666235715\n",
      "Ridge Simple MAE: 0.9828045748451621\n",
      "Lasso Simple MAE: 0.9845237667499999\n"
     ]
    }
   ],
   "source": [
    "LR = LinearRegression()\n",
    "RR = Ridge(alpha=1000)\n",
    "LassoR = Lasso(alpha=1000)\n",
    "\n",
    "LR.fit(X_simple_train, y_train)\n",
    "LR_pred = LR.predict(X_simple_test)\n",
    "\n",
    "RR.fit(X_simple_train, y_train)\n",
    "RR_pred = RR.predict(X_simple_test)\n",
    "\n",
    "LassoR.fit(X_simple_train, y_train)\n",
    "LassoR_pred = LassoR.predict(X_simple_test)\n",
    "\n",
    "print(f'Linear Simple MAE: {mean_absolute_error(y_test, LR_pred)}')\n",
    "print(f'Ridge Simple MAE: {mean_absolute_error(y_test, RR_pred)}')\n",
    "print(f'Lasso Simple MAE: {mean_absolute_error(y_test, LassoR_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c984cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------METRICS REPORT-----------------\n",
      "\n",
      "MSE: 1.507\n",
      "RMSE: 1.228\n",
      "MAE: 0.982\n",
      "R2: 0.974\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\4256537210.py:161: UserWarning: Ramsey Test failed, your model has non-linear relations, try using polynomial or logarithmic convertions.\n",
      "  warnings.warn('Ramsey Test failed, your model has non-linear relations, try using polynomial or logarithmic convertions.')\n",
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\4256537210.py:234: UserWarning: Jarque Bera test failed. The residuals are not normally distributed.\n",
      "  warnings.warn('Jarque Bera test failed. The residuals are not normally distributed.')\n",
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\4256537210.py:206: UserWarning: The BP test shows that your model has heterocedaskicity problems, try transforming the dependent and independent variables.\n",
      "  warnings.warn('The BP test shows that your model has heterocedaskicity problems, try transforming the dependent and independent variables.')\n"
     ]
    }
   ],
   "source": [
    "Juan_LR = LinearRegressor()\n",
    "Juan_LR.fit(X_simple_train, y_train)\n",
    "pred_Juan_simple = Juan_LR.predict(X_simple_test)\n",
    "Juan_LR.get_metrics_report(X_simple_test, pred_Juan_simple, y_test, charts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2ecff524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Simple MAE: 0.5183922114609337\n",
      "Ridge Simple MAE: 0.52261892925488\n",
      "Lasso Simple MAE: 0.9845237667499999\n"
     ]
    }
   ],
   "source": [
    "LR.fit(X_mult_train, y_train)\n",
    "LR_pred_mult = LR.predict(X_mult_test)\n",
    "\n",
    "RR.fit(X_mult_train, y_train)\n",
    "RR_pred_mult = RR.predict(X_mult_test)\n",
    "\n",
    "LassoR.fit(X_mult_train, y_train)\n",
    "LassoR_pred_mult = LassoR.predict(X_mult_test)\n",
    "\n",
    "print(f'Linear Simple MAE: {mean_absolute_error(y_test, LR_pred_mult)}')\n",
    "print(f'Ridge Simple MAE: {mean_absolute_error(y_test, RR_pred_mult)}')\n",
    "print(f'Lasso Simple MAE: {mean_absolute_error(y_test, LassoR_pred_mult)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "da5ecbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! HouseAge has a low correlation with the target variable. Corr: 0.11\n",
      "WARNING! AveRooms has a low correlation with the target variable. Corr: 0.14\n",
      "WARNING! AveBedrms has a low correlation with the target variable. Corr: -0.04\n",
      "WARNING! Population has a low correlation with the target variable. Corr: -0.03\n",
      "WARNING! AveOccup has a low correlation with the target variable. Corr: -0.05\n",
      "WARNING! Latitude has a low correlation with the target variable. Corr: -0.18\n",
      "WARNING! Longitude has a low correlation with the target variable. Corr: 0.02\n",
      "\n",
      "------------------METRICS REPORT-----------------\n",
      "\n",
      "MSE: 0.496\n",
      "RMSE: 0.704\n",
      "MAE: 0.52\n",
      "R2: 0.663\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\611525331.py:151: UserWarning: Ramsey Test failed, your model has non-linear relations, try using polynomial or logarithmic convertions.\n",
      "  warnings.warn('Ramsey Test failed, your model has non-linear relations, try using polynomial or logarithmic convertions.')\n",
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\611525331.py:216: UserWarning: Jarque Bera test failed. The residuals are not normally distributed.\n",
      "  warnings.warn('Jarque Bera test failed. The residuals are not normally distributed.')\n",
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\611525331.py:191: UserWarning: The BP test shows that your model has heterocedaskicity problems, try transforming the dependent and independent variables.\n",
      "  warnings.warn('The BP test shows that your model has heterocedaskicity problems, try transforming the dependent and independent variables.')\n",
      "C:\\Users\\juans\\AppData\\Local\\Temp\\ipykernel_15756\\611525331.py:239: UserWarning: Multicolinearity test failed, This is a serious issue, check the correlation between your features, they should not be correlated.\n",
      "  warnings.warn('Multicolinearity test failed, This is a serious issue, check the correlation between your features, they should not be correlated.')\n"
     ]
    }
   ],
   "source": [
    "Juan_LR = LinearRegressor()\n",
    "Juan_LR.fit(X_mult_train, y_train)\n",
    "pred_Juan_mult = Juan_LR.predict(X_mult_test)\n",
    "Juan_LR.get_metrics_report(X_mult_test, pred_Juan_mult, y_test, charts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5df564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Validator:\n",
    "\n",
    "#     def validate(self, X, y):\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    @staticmethod\n",
    "    def _convert_data(X: pd.DataFrame | pd.Series, y: pd.Series, train=True) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        if isinstance(X, pd.Series):\n",
    "            X = X.values.reshape(-1, 1)\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        if train:\n",
    "            if isinstance(y, pd.Series):\n",
    "                y = y.values.reshape(-1, 1)\n",
    "            elif isinstance(y, pd.DataFrame):\n",
    "                y = y.values\n",
    "\n",
    "            return X, y\n",
    "        \n",
    "        return X\n",
    "    \n",
    "\n",
    "class CorrChecker:\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_corr(X: pd.Series | pd.DataFrame, y: pd.Series) -> None:\n",
    "\n",
    "        df = pd.concat([X, y], axis=1)\n",
    "        corrs = df.corr().iloc[:, -1]\n",
    "\n",
    "        for col, corr in zip(corrs.index, corrs):\n",
    "            corr = np.round(corr, 2)\n",
    "            if abs(corr) < 0.3:\n",
    "                print(f'WARNING! {col} has a low correlation with the target variable. Corr: {corr}')\n",
    "\n",
    "\n",
    "class LinearRegressor:\n",
    "\n",
    "    def __init__(self, w=None, b=None, TimeSeries=False):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.TimeSeries = TimeSeries\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_weights(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        b = np.ones(shape=(X.shape[0], 1))\n",
    "        input_b = np.hstack((X, b))\n",
    "\n",
    "        weights = (np.linalg.inv(input_b.T @ input_b) @ input_b.T) @ y\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame | pd.Series, y_train: pd.Series) -> None:\n",
    "\n",
    "        '''\n",
    "            This method acts like training the model, it will also tell you if your\n",
    "            model has heterocedaskicity, autocorrelation, multicolinearity, is it\n",
    "            bad specified and if the data is normally distributed.\n",
    "        '''\n",
    "\n",
    "        CorrChecker._check_corr(X_train, y_train)\n",
    "        X_train_r, y_train_r = Preprocessor._convert_data(X_train, y_train)\n",
    "        w_b = self._get_weights(X_train_r, y_train_r)\n",
    "        self.w = w_b[:-1]\n",
    "        self.b = w_b[-1]\n",
    "        print()\n",
    "        \n",
    "        AssumpChecker._check_assumptions(X_train_r, y_train_r, self.w, self.b, self.TimeSeries)\n",
    "\n",
    "        if X_train.shape[1] > 1 and len(X_train.shape) > 1:\n",
    "            AssumpChecker._check_multicol(X_train)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame | pd.Series) -> np.ndarray:\n",
    "\n",
    "        '''\n",
    "            This method predicts the test set using the weights and the Bias.\n",
    "        '''\n",
    "\n",
    "        X = Preprocessor._convert_data(X, np.array([1]), train=False)\n",
    "\n",
    "        pred = X @ self.w + self.b\n",
    "        return pred\n",
    "\n",
    "    def get_metrics_report(self,\n",
    "                           X: pd.DataFrame | pd.Series,\n",
    "                           pred: np.ndarray,\n",
    "                           y: np.ndarray,\n",
    "                           charts=True) -> None:\n",
    "\n",
    "        '''\n",
    "            This method will give you a quick report of you regression, showing you metrics\n",
    "            like RMSE, MSE, MAE, R2, a regression plot and a residual plot.\n",
    "        '''\n",
    "\n",
    "        X_res, y_res = Preprocessor._convert_data(X, y)\n",
    "\n",
    "        RMSE = Metrics.get_RMSE(pred, y_res)\n",
    "        MSE = Metrics.get_MSE(pred, y_res)\n",
    "        MAE = Metrics.get_MAE(pred, y_res)\n",
    "        R2 = Metrics.get_R2(pred, y_res)\n",
    "        residuals = y_res - pred\n",
    "\n",
    "        print('------------------METRICS REPORT-----------------\\n')\n",
    "        print(f'MSE: {np.round(MSE, 3)}\\nRMSE: {np.round(RMSE, 3)}\\nMAE: {np.round(MAE, 2)}\\nR2: {np.round(R2, 3)}\\n')\n",
    "\n",
    "        if charts:\n",
    "\n",
    "            if X_res.shape[1] == 1:\n",
    "                Report._get_regplot(X_res, pred, y_res)\n",
    "                print()\n",
    "\n",
    "            Report._get_residual_plot(pred, residuals)\n",
    "    \n",
    "    \n",
    "class AssumpChecker:\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_Ramsey(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: np.ndarray) -> None:\n",
    "\n",
    "        ''' \n",
    "            This method does all the Ramsey process from start to finish, it\n",
    "            can be hard to understand, so I suggest supporting the explanations\n",
    "            with AI.\n",
    "        '''\n",
    "\n",
    "        w1, b1 = w, b\n",
    "        pred1 = X @ w1 + b1\n",
    "        pred1_sq = pred1 ** 2\n",
    "        pred1_cu = pred1 ** 3\n",
    "        ssrr = np.sum((y - pred1)**2, axis=0)\n",
    "\n",
    "        X2 = np.hstack([X, pred1_sq, pred1_cu])\n",
    "        all_w_R = LinearRegressor._get_weights(X2, y)\n",
    "        w2, b2 = all_w_R[0:-1], all_w_R[-1]\n",
    "        pred2 = X2 @ w2 + b2\n",
    "        ssra = np.sum((y - pred2)**2, axis=0)\n",
    "\n",
    "        q = 2\n",
    "        n = X.shape[0]\n",
    "        k = X.shape[1]\n",
    "\n",
    "        F = ((ssrr-ssra) / q) / (ssra / (n - k - q - 1)) \n",
    "        F = F[0]\n",
    "        F_critic = scipy.stats.f.ppf(1-0.05, k, n-k-q-1)\n",
    "\n",
    "        if F > F_critic:\n",
    "            warnings.warn('Ramsey Test failed, your model has non-linear relations, try using polynomial or logarithmic convertions.')\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_dw(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: np.ndarray) -> None:\n",
    "\n",
    "        ''' This method just applies for Time Series data only, it does the Durbin Watson test. '''\n",
    "\n",
    "        pred = X @ w + b\n",
    "        res = y - pred\n",
    "        d = np.sum((res[:-1] - res[1:])**2) / np.sum(res **2)\n",
    "\n",
    "        if d < 1.8:\n",
    "            warnings.warn('Durbin Watson test shows that the model has positive correlation problems, try adding lags of one of the dependent variables as another dependent variable.')\n",
    "        elif d > 2.2:\n",
    "            warnings.warn('Durbin Watson test shows that the model has negative correlation problems, try adding lags of one of the dependent variables as another dependent variable.')\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_ht(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: np.ndarray) -> None:\n",
    "\n",
    "        ''' This test checks for Heterocedaskicity on your model.'''\n",
    "\n",
    "        pred = X @ w + b\n",
    "        res = y - pred\n",
    "        res_sq = res**2\n",
    "        n = X.shape[0]\n",
    "        k = X.shape[1]\n",
    "        ssr = np.sum(res_sq)\n",
    "\n",
    "        var_res = ssr / (n-k-1)\n",
    "        g =  res_sq / var_res\n",
    "\n",
    "        all_w2 = LinearRegressor._get_weights(X, g)\n",
    "        w2, b2 = all_w2[:-1], all_w2[-1]\n",
    "        pred2 = X @ w2 + b2\n",
    "\n",
    "        R2 = Metrics.get_R2(pred2, g)\n",
    "        LM = n * R2\n",
    "        chi_square_value = scipy.stats.chi2.ppf(1-0.05, k)\n",
    "\n",
    "        if LM > chi_square_value:\n",
    "            warnings.warn('The BP test shows that your model has heterocedaskicity problems, try transforming the dependent and independent variables.')\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_jb(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: np.ndarray) -> None:\n",
    "\n",
    "        ''' This is a private method '''\n",
    "\n",
    "        pred = X @ w + b\n",
    "        res = y - pred\n",
    "        n = X.shape[0]\n",
    "        res_mean = np.mean(res)\n",
    "\n",
    "        M2 = 1/n * np.sum((res-res_mean)**2)\n",
    "        M3 = 1/n * np.sum((res-res_mean)**3)\n",
    "        M4 = 1/n * np.sum((res-res_mean)**4)\n",
    "        std_dev = np.sqrt(M2)\n",
    "        \n",
    "        S = M3 / std_dev**3\n",
    "        K = M4 / std_dev**4\n",
    "\n",
    "        JB = n/6 * ((S**2) + ((K - 3)**2 / 4))\n",
    "\n",
    "        JB_critic = scipy.stats.chi2.ppf(1-0.05, 2)\n",
    "\n",
    "        if JB > JB_critic:\n",
    "            warnings.warn('Jarque Bera test failed. The residuals are not normally distributed.')\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_multicol(X: pd.DataFrame) -> None:\n",
    "\n",
    "        ''' This is a private method '''\n",
    "\n",
    "        VIFs = {col:0 for col in X.columns}\n",
    "\n",
    "        for col in X.columns:\n",
    "            X_aux = X.drop(col, axis=1)\n",
    "            y = X[col]\n",
    "\n",
    "            all_w = LinearRegressor._get_weights(X_aux.values, y.values)\n",
    "            w, b = all_w[:-1], all_w[-1]\n",
    "            pred = X_aux @ w + b\n",
    "            R2_aux = Metrics.get_R2(pred, y)\n",
    "\n",
    "            VIFs[col] = 1 / (1 - R2_aux)\n",
    "\n",
    "        for vif in VIFs.values():\n",
    "\n",
    "            if vif > 5:\n",
    "                warnings.warn('Multicolinearity test failed, This is a serious issue, check the correlation between your features, they should not be correlated.')\n",
    "                break\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_assumptions(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: np.ndarray, TimeSeries = False) -> None:\n",
    "\n",
    "        ''' This is a private method '''\n",
    "\n",
    "        AssumpChecker._check_Ramsey(X, y, w, b)\n",
    "        AssumpChecker._check_jb(X, y, w, b)\n",
    "        AssumpChecker._check_ht(X, y, w, b)\n",
    "\n",
    "        if TimeSeries:\n",
    "            AssumpChecker._check_dw(X, y, w, b)\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_MSE(pred: np.ndarray, y: np.ndarray) -> float:\n",
    "\n",
    "        ''' This method calculates the MSE '''\n",
    "\n",
    "        m = pred.shape[0]\n",
    "\n",
    "        MSE = (1/m) * np.sum((y - pred) ** 2)\n",
    "        return MSE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_RMSE(pred: np.ndarray, y: np.ndarray) -> float:\n",
    "\n",
    "        ''' This method calculates the RMSE '''\n",
    "\n",
    "        m = pred.shape[0]\n",
    "\n",
    "        RMSE = ((1/m) * np.sum((y - pred) ** 2)) ** 0.5\n",
    "        return RMSE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_MAE(pred: np.ndarray, y: np.ndarray) -> float:\n",
    "\n",
    "        ''' This method calculates the MAE '''\n",
    "\n",
    "        m = pred.shape[0]\n",
    "\n",
    "        MAE = 1/m * (np.sum(abs(y - pred)))\n",
    "        return MAE\n",
    "\n",
    "    @staticmethod\n",
    "    def get_R2(pred: np.ndarray, y: np.ndarray) -> float:\n",
    "\n",
    "        ''' This method calculates the R2 '''\n",
    "\n",
    "        num = np.sum((y - pred) ** 2)\n",
    "        den = np.sum((y - y.mean()) ** 2)\n",
    "        R2 = 1 - (num / den)\n",
    "\n",
    "        return R2\n",
    "\n",
    "\n",
    "class Report:\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_regplot(X: np.ndarray, pred: np.ndarray, y: np.ndarray) -> None:\n",
    "\n",
    "        ''' This is a private method '''\n",
    "\n",
    "        plt.scatter(y, X, alpha=0.3)\n",
    "        plt.plot(pred, X, c='r')\n",
    "        plt.title('Regresion plot')\n",
    "        plt.xlabel('Input')\n",
    "        plt.ylabel('Actual Values')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_residual_plot(pred: np.ndarray, residuals: np.ndarray) -> None:\n",
    "\n",
    "        ''' This is a private method '''\n",
    "\n",
    "        plt.scatter(pred, residuals)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel('Predictions')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.title('Residuals Plot')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class RidgeRegressor(LinearRegressor):\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    @classmethod\n",
    "    def _get_weights(self, input: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        b = np.ones(shape=(input.shape[0], 1))\n",
    "        input_b = np.hstack((input, b))\n",
    "\n",
    "        weights = (np.linalg.inv((input_b.T @ input_b) + self.alpha * np.eye(input_b.shape[1])) @ input_b.T) @ y\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class LinearRegressor:\n",
    "\n",
    "    def __init__(self, w=None, b=None):\n",
    "        self.__w = w\n",
    "        self.__b = b\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
